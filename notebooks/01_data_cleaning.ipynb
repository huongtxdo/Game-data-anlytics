{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe16fb5",
   "metadata": {},
   "source": [
    "Clean and validate data\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c35d9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_root = os.path.abspath(\"..\")\n",
    "reg_path = os.path.join(project_root, \"data\", \"raw\", \"reg_data.csv\")\n",
    "auth_path = os.path.join(project_root, \"data\", \"raw\", \"auth_data.csv\")\n",
    "ab_path = os.path.join(project_root, \"data\", \"raw\", \"ab_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc863ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "273c3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_data = pd.read_csv(reg_path, delimiter=\";\")\n",
    "df_auth_data = pd.read_csv(auth_path, delimiter=\";\")\n",
    "df_ab_test = pd.read_csv(ab_path, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c1b3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate found in df_reg_data\n",
      "No duplicate found in df_auth_data\n",
      "No duplicate found in df_ab_test\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_reg_data, df_auth_data, df_ab_test,]\n",
    "df_names = [\"df_reg_data\", \"df_auth_data\", \"df_ab_test\"]\n",
    "for i in range(0, len(dfs)):\n",
    "    if dfs[i].duplicated().sum() != 0:\n",
    "        print(\"Duplicates found in \", df_names[i])\n",
    "    else:\n",
    "        print(f\"No duplicate found in {df_names[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3e6e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix Timestamp to datetime\n",
    "df_reg_data[\"reg_ts\"] = pd.to_datetime(df_reg_data[\"reg_ts\"], utc=True, unit=\"s\")\n",
    "df_auth_data[\"auth_ts\"] = pd.to_datetime(df_auth_data[\"auth_ts\"], utc=True, unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42349916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     reg_ts  uid\n",
      "0 1998-11-18 09:43:43+00:00    1\n",
      "1 1999-07-22 22:38:09+00:00    2\n",
      "2 2000-01-13 22:27:27+00:00    3\n",
      "3 2000-05-28 14:19:01+00:00    4\n",
      "4 2000-09-16 11:21:53+00:00    5\n",
      "===========\n",
      "\n",
      "                    auth_ts  uid\n",
      "0 1998-11-18 09:43:43+00:00    1\n",
      "1 1999-07-22 22:38:09+00:00    2\n",
      "2 1999-07-25 16:46:46+00:00    2\n",
      "3 1999-07-31 03:50:15+00:00    2\n",
      "4 1999-08-05 17:49:39+00:00    2\n"
     ]
    }
   ],
   "source": [
    "print(df_reg_data.head(5))\n",
    "print(\"===========\")\n",
    "print(df_auth_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddaa6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cleaned dataframe is saved as 2 copies: \n",
    "## csv for cross platform and readability\n",
    "reg_data_csv = os.path.join(project_root, \"data\", \"processed\", \"clean_reg_data.csv\")\n",
    "df_reg_data.to_csv(reg_data_csv, index=False)\n",
    "auth_data_csv = os.path.join(project_root, \"data\", \"processed\", \"clean_auth_data.csv\")\n",
    "df_auth_data.to_csv(auth_data_csv, index=False)\n",
    "ab_test_csv = os.path.join(project_root, \"data\", \"processed\", \"clean_ab_test.csv\")\n",
    "df_ab_test.to_csv(ab_test_csv, index=False)\n",
    "## pkl for fast loading and type preservation\n",
    "reg_data_pkl = os.path.join(project_root, \"data\", \"processed\", \"clean_reg_data.pkl\")\n",
    "df_reg_data.to_pickle(reg_data_pkl)\n",
    "auth_data_pkl = os.path.join(project_root, \"data\", \"processed\", \"clean_auth_data.pkl\")\n",
    "df_auth_data.to_pickle(auth_data_pkl)\n",
    "ab_test_pkl= os.path.join(project_root, \"data\", \"processed\", \"clean_ab_test.pkl\")\n",
    "df_ab_test.to_pickle(ab_test_pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
